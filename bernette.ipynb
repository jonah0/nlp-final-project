{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711080d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jonah/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0098d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./test-datasets/sentiment-topic-final-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdccf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = data['text'].to_list()\n",
    "gold = data['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556ddf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf9f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores\n",
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ec88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.33      0.50         3\n",
      "     neutral       1.00      0.33      0.50         3\n",
      "    positive       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.83      0.56      0.56        10\n",
      "weighted avg       0.80      0.60      0.57        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vader on default\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b66596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.33      0.40         3\n",
      "     neutral       0.00      0.00      0.00         3\n",
      "    positive       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.33      0.44      0.36        10\n",
      "weighted avg       0.35      0.50      0.39        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# vader lemmatize\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, lemmatize=True))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc37cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         3\n",
      "     neutral       0.43      1.00      0.60         3\n",
      "    positive       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.37      0.50      0.39        10\n",
      "weighted avg       0.40      0.50      0.41        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# vader adjectives\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'ADJ'}))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077deddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         3\n",
      "     neutral       0.43      1.00      0.60         3\n",
      "    positive       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.37      0.50      0.39        10\n",
      "weighted avg       0.40      0.50      0.41        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# vader adjectives lemmatize\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'ADJ'}, lemmatize=True))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf94ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.33      0.50         3\n",
      "     neutral       0.40      0.67      0.50         3\n",
      "    positive       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.63      0.50      0.50        10\n",
      "weighted avg       0.62      0.50      0.50        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vader verb\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'VERB'}))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab94627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.33      0.40         3\n",
      "     neutral       0.25      0.33      0.29         3\n",
      "    positive       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.42      0.39      0.40        10\n",
      "weighted avg       0.42      0.40      0.41        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vader verb lemmatize\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'VERB'}, lemmatize=True))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1113e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.33      0.50         3\n",
      "     neutral       0.38      1.00      0.55         3\n",
      "    positive       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.46      0.44      0.35        10\n",
      "weighted avg       0.41      0.40      0.31        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vader noun lemmatize\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'NOUN'}, lemmatize=True))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990d6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.33      0.50         3\n",
      "     neutral       0.33      1.00      0.50         3\n",
      "    positive       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.44      0.44      0.33        10\n",
      "weighted avg       0.40      0.40      0.30        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonah/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# vader noun\n",
    "vader = []\n",
    "for review in reviews:\n",
    "    scores = vader_output_to_label(run_vader(review, parts_of_speech_to_consider={'NOUN'}))\n",
    "    vader.append(scores)\n",
    "\n",
    "print(classification_report(gold, vader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
